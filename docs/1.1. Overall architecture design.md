# luacluster overall architecture design

## One, the overall architecture design

![](framework-1.png)

The overall architecture of luacluster is very simple. Node corresponds to process, docker corresponds to thread, each docker has a luavm and a message queue, and entity objects are created in luavm. net network and log logs run separately in separate threads. Very concise and clear without any superfluous cumbersome stuff.

**The design goal of luacluster is to allow any entity to be asynchronously invoked through rpc. The so-called rpc method is to use the entity's id, function name, and parameters in any entity to asynchronously call any entity. **

luacluster is a distributed and parallel system. Distributed means multiple processes, and parallel means multiple threads. That is to say, luacluster wants to achieve a penetrating process and thread blocking. It can make any entities call each other like a normal system interface. Which luacluster must be an asynchronous system. That is to say, any function calls between luacluster entities are asynchronous.

For example the part in the bigworld object that informs the entity to enter the sudoku space

````lua
local entityProxy = udpproxy.New(id)
entityProxy:OnEntryWorld(self.spaceType, self.beginx, self.beginz, self.endx, self.endz)
````

It is to create a remote object proxy through the entity id. Then call the account's OnEntryWorld function through the remote object proxy. If the entity is in the current process, it will look up the docker id and post it to the specified message queue. If it is on other nodes, it will be sent using the udp protocol. The trick to find the corresponding entity by id is because the ip address, docker id, and entity id are all stuffed into this unint64. You can find the definition of entity id in entityid.h.

````
typedef struct _EID {
unsigned short id;
unsigned int addr;//ipv4
unsigned char dock;
unsigned char port;//Offset of UDP port number
}*PEID, EID;

typedef union idl64
{
volatile EID eid;
volatile unsigned long long u;
volatile double d;
} idl64;
````

In this way we have achieved a very amazing and efficient asynchronous communication system. Object communication in any process and thread can be accomplished in at most 2 steps. Find the upd port and send it, find the thread queue and send it. In any environment, as long as you get the entity id, you can quickly know the destination of the packet. ** Implement calls like normal function calls between any objects within the distributed network. **What network programming, what multi-line programming can all go to hell.

## Second, about the news storm.

As the name suggests, 10,000 people on the same screen needs to deal with the location synchronization problem of 10,000 players on the server. Syncing the positions of 10,000 players generates 100 million messages at a time. 10,000 by 10,000 produces 100 million messages. Keep in mind that 100 million will be mentioned again and again later.

First, let's analyze the generation process of 100 million messages. The server will receive 10,000 client mobile requests. 10,000 requests is no problem, and now the server handles 100,000 links without a problem. Therefore, the general server pressure for these 10,000 requests is not large. The problem is that each of these 10,000 requests will generate 10,000 new requests to send to other players. The server can't handle it. A 100 million io demand is generated at once. No message queue can handle it, and the mutex is directly locked.

So I used the CreateMsgList interface to create a message list. Any io request is transformed into an operation of inserting 10,000 lists. Then merge this list with the message queue. In this way, 100 million io requests become 10,000 io requests, and the io requests are compressed by 10,000 times. In the same way, we can also compress the packets sent to the client. Handling 100 million packet requests is difficult but handling 10,000 packets is much less difficult. The following is part of the code for packet compression.

````c
void DockerSendToClient(void* pVoid, unsigned long long did, unsigned long long pid, const char* pc, size_t s) {

if (pVoid == 0)return;
PDockerHandle pDockerHandle = pVoid;
idl64 eid;
eid.u = pid;
unsigned int addr = eid.eid.addr;
unsigned char port = ~eid.eid.port;
unsigned char docker = eid.eid.dock;

unsigned int len ​​= sizeof(ProtoRoute) + s;
PProtoHead pProtoHead = malloc(len);
if (pProtoHead == 0)
return;
...
    
    sds pbuf = dictGetVal(entryCurrent);
    size_t l = sdslen(pbuf) + len;
    if (l > pDocksHandle->packetSize) {
        DockerSendPacket(pid, pbuf);
        sdsclear(pbuf);
    }
    if (sdsavail(pbuf) < len) {
        pbuf = sdsMakeRoomFor(pbuf, len);

        if (pbuf == NULL) {
            n_error("DockerSendToClient2 sdsMakeRoomFor is null");
            return;
        }

        dictSetVal(pDockerHandle->entitiesCache, entryCurrent, pbuf);
    }
    memcpy(pbuf + sdslen(pbuf), pProtoHead, len);
    sdsIncrLen(pbuf, (int)len);

    pDockerHandle->stat_packet_count++;
````

Of course packet compression doesn't solve all our problems. The challenge of handling 100 million requests and compressing them is also enormous. On a 128-core server, each core can only handle 200,000 requests per second. Each lua operation is an operation on a hashmap, and the hashmap insertion operation is about 200,000 times per second. And 100 million requests require 780,000 processing power. 100 million requests on 128 cores take about 4 seconds to complete. Or... only process 20% of players per second. This means that we can only guarantee that 20% of player requests are processed per second. In this way, we will sacrifice another big killer of our "state synchronization". We want to describe the player's movement as a state over a period of time. Complete status with position, direction, speed, start time, stop time. In this way, each client can infer the correct state of the player's movement based on this information.

````lua
#Part of the code to synchronize the player state in the MoveTo(x, y, z) function
local list = docker.CreateMsgList()
​ for k, v1 in pairs(self.entities) do
​ local v = entitymng.EntityDataGet(k)
​ if v ~= nil then
​ local view = udpproxylist.New(v[1], list)
​ view:OnMove(self.id, self.transform.position.x, self.transform.position.y, self.transform.position.z
​ ,self.transform.rotation.x, self.transform.rotation.y, self.transform.rotation.z, self.transform.velocity
​ , self.transform.stamp, self.transform.stampStop)
​ end
​ end
​ docker.PushAllMsgList(list)
​ docker.DestoryMsgList(list)
````

This way we can guarantee that less than 20% of player requests need to be processed in any state. But don't be optimistic although the problem of moving can be solved. But when a new player enters the scene, it is also necessary to synchronize the data of all players and synchronize the data of the new player to other players. If there are 5,000 players in the scene, put another 5,000 players. The data that the players on both sides need to synchronize is "5,000 * 5,000 + 5,000 * 5,000" for a total of 50 million. Although it is less than half of 100 million, the 128-core server analyzed earlier can only handle 25.6 million. "We can go to 258 core servers", "oh yes yes yes".

Of course, there is no need to use a 256 core server. We can change the way, that is, only 500 people are put in at a time. In this way, the data that needs to be synchronized becomes 500 * 10,000 + 10,000 * 500, 10 million times at most. This will meet our hardware needs.

## Three, the concept
